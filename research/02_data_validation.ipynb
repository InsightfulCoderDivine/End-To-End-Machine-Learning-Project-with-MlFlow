{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Documents\\\\EndToEndMLProjects\\\\End-To-End-Machine-Learning-Project-with-MlFlow\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Documents\\\\EndToEndMLProjects\\\\End-To-End-Machine-Learning-Project-with-MlFlow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Update config.yaml\n",
    "\n",
    "- Open config/config.yaml file and add data_validation configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Update schema.yaml\n",
    "\n",
    "* Contains:\n",
    "- How many columns your data is containing\n",
    "- Name and dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "loan_df = pd.read_csv('artifacts/data_ingestion/Loan_Default.csv')\n",
    "loan_df_copy = loan_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_df['Credit_Score'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: int64\n",
      "year: int64\n",
      "loan_limit: object\n",
      "Gender: object\n",
      "approv_in_adv: object\n",
      "loan_type: object\n",
      "loan_purpose: object\n",
      "Credit_Worthiness: object\n",
      "open_credit: object\n",
      "business_or_commercial: object\n",
      "loan_amount: int64\n",
      "rate_of_interest: float64\n",
      "Interest_rate_spread: float64\n",
      "Upfront_charges: float64\n",
      "term: float64\n",
      "Neg_ammortization: object\n",
      "interest_only: object\n",
      "lump_sum_payment: object\n",
      "property_value: float64\n",
      "construction_type: object\n",
      "occupancy_type: object\n",
      "Secured_by: object\n",
      "total_units: object\n",
      "income: float64\n",
      "credit_type: object\n",
      "Credit_Score: int64\n",
      "co-applicant_credit_type: object\n",
      "age: object\n",
      "submission_of_application: object\n",
      "LTV: float64\n",
      "Region: object\n",
      "Security_Type: object\n",
      "Status: int64\n",
      "dtir1: float64\n"
     ]
    }
   ],
   "source": [
    "for col in loan_df.columns:\n",
    "    print(f'{col}: {loan_df[col].dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Update entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidatioConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data validation.\n",
    "\n",
    "    This dataclass stores the configuration details required for validating data.\n",
    "    The `frozen=True` parameter ensures that instances of this class are immutable.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory where data and related files are stored.\n",
    "        unzip_data_dir (Path): Directory where the unzipped data files are located.\n",
    "        STATUS_FILE (str): Name or path of the file that tracks the status of the data validation process.\n",
    "        all_schema (dict): Dictionary containing the schema details for data validation.\n",
    "                           Typically includes keys and expected data types for the datasets.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    unzip_data_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    all_schema: dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Update the configuration manager in src config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlproject.constants import *\n",
    "from mlproject.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Manages the configuration and setup for the project.\n",
    "\n",
    "    This class is responsible for reading configuration files, creating required directories, \n",
    "    and providing specific configuration objects needed for various components of the project.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed content of the main configuration file.\n",
    "        params (dict): Parsed content of the parameters file.\n",
    "        schema (dict): Parsed content of the schema file.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager.\n",
    "\n",
    "        Reads YAML configuration files for main configuration, parameters, and schema. \n",
    "        Also ensures that the root artifacts directory specified in the configuration is created.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): Path to the main configuration YAML file. Default is `CONFIG_FILE_PATH`.\n",
    "            params_filepath (str): Path to the parameters YAML file. Default is `PARAMS_FILE_PATH`.\n",
    "            schema_filepath (str): Path to the schema YAML file. Default is `SCHEMA_FILE_PATH`.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_validation_config(self) -> DataValidatioConfig:\n",
    "        \"\"\"\n",
    "        Creates and returns a `DataValidatioConfig` object for data validation.\n",
    "\n",
    "        This method retrieves the data validation-specific configuration from the main \n",
    "        configuration and schema files. It also ensures the directories required for \n",
    "        data validation are created.\n",
    "\n",
    "        Returns:\n",
    "            DataValidatioConfig: An instance of `DataValidatioConfig` initialized with \n",
    "            the appropriate paths and schema information.\n",
    "        \"\"\"\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.COLUMNS\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_validation_config = DataValidatioConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            unzip_data_dir=config.unzip_data_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            all_schema=schema\n",
    "        )\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Update the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlproject import logger\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    \"\"\"Performs data validation checks based on the provided configuration.\n",
    "\n",
    "    This class validates the columns and data types of a dataset against a predefined schema. \n",
    "    It checks whether all expected columns are present and whether their data types match the schema.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataValidatioConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataValidation object with the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            config (DataValidatioConfig): The configuration object containing the schema \n",
    "                                          and file paths for data validation.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        \n",
    "    def validate_all_columns(self) -> Tuple[bool, bool, bool]:\n",
    "        \"\"\"\n",
    "        Validates the dataset against the predefined schema.\n",
    "\n",
    "        This method checks:\n",
    "            1. If all columns in the dataset match the schema.\n",
    "            2. If the data types of the columns in the dataset match the expected types in the schema.\n",
    "            3. If there are duplicate rows in the dataset.\n",
    "\n",
    "        The validation results are written to the status file specified in the configuration.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[bool, bool, bool]: A tuple containing three boolean values:\n",
    "            - First value: True if all columns match the schema, False otherwise.\n",
    "            - Second value: True if all data types match the schema, False otherwise.\n",
    "            - Third value: True if no duplicate rows exist, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the validation process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validation_status_col = None\n",
    "            validation_status_dtype = None\n",
    "            validation_status_duplicates = None\n",
    "\n",
    "            data = pd.read_csv(self.config.unzip_data_dir)\n",
    "            all_cols = list(data.columns)\n",
    "            all_dtypes = list(data.dtypes)\n",
    "            \n",
    "            all_schema = self.config.all_schema.keys()\n",
    "            all_schema_dtypes = self.config.all_schema.values()\n",
    "            \n",
    "            # Validate columns\n",
    "            for col in all_cols:\n",
    "                if col not in all_schema:\n",
    "                    validation_status_col = False\n",
    "                else:\n",
    "                    validation_status_col = True\n",
    "\n",
    "            # Validate data types\n",
    "            for dtype in all_dtypes:\n",
    "                if dtype not in all_schema_dtypes:\n",
    "                    validation_status_dtype = False\n",
    "                else:\n",
    "                    validation_status_dtype = True\n",
    "                    \n",
    "            # Check for Duplicate rows\n",
    "            if data.duplicated().any():\n",
    "                # 'No duplicate rows: False'\n",
    "                validation_status_duplicates = False\n",
    "            else:\n",
    "                # 'No duplicate rows: True'\n",
    "                validation_status_duplicates = True\n",
    "\n",
    "            # Write final results\n",
    "            with open(self.config.STATUS_FILE, \"w\") as f:\n",
    "                f.write(f\"Columns Validation status: {validation_status_col}\\n\")\n",
    "                f.write(f\"Dtypes Validation status: {validation_status_dtype}\\n\")\n",
    "                f.write(f\"No Duplicate Rows status: {validation_status_duplicates}\\n\")\n",
    "\n",
    "            return validation_status_col, validation_status_dtype, validation_status_duplicates\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Update the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-20 02:12:53,263: 35 mlprojectLogger: INFO: common: .yaml file: config\\config.yaml loaded successfully.]\n",
      "[2024-11-20 02:12:53,266: 35 mlprojectLogger: INFO: common: .yaml file: params.yaml loaded successfully.]\n",
      "[2024-11-20 02:12:53,274: 35 mlprojectLogger: INFO: common: .yaml file: schema.yaml loaded successfully.]\n",
      "[2024-11-20 02:12:53,276: 54 mlprojectLogger: INFO: common: Created directory at artifacts]\n",
      "[2024-11-20 02:12:53,278: 54 mlprojectLogger: INFO: common: Created directory at artifacts/data_validation]\n"
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_validation_config()\n",
    "    data_validation = DataValidation(data_validation_config)\n",
    "    data_validation.validate_all_columns()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "* Performed in `research/EDA.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
