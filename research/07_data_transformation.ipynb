{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Documents\\\\EndToEndMLProjects\\\\End-To-End-Machine-Learning-Project-with-MlFlow\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Documents\\\\EndToEndMLProjects\\\\End-To-End-Machine-Learning-Project-with-MlFlow'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Update config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Update the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for data transformation in a machine learning pipeline.\n",
    "\n",
    "    This dataclass holds paths for training and testing datasets, both features (X) \n",
    "    and labels (y), required during the data transformation process. The `frozen=True` \n",
    "    parameter ensures immutability of the configuration object.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory for storing all data transformation outputs.\n",
    "        X_train_data_path (str): Path to the file containing the training feature data.\n",
    "        X_test_data_path (str): Path to the file containing the testing feature data.\n",
    "        y_train_data_path (str): Path to the file containing the training labels.\n",
    "        y_test_data_path (str): Path to the file containing the testing labels.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    X_train_data_path: str\n",
    "    X_test_data_path: str\n",
    "    y_train_data_path: str\n",
    "    y_test_data_path: str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Update the configuration manager in src config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlproject.constants import *\n",
    "from mlproject.utils.common import create_directories, read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    Manages the configuration and setup for the project.\n",
    "\n",
    "    This class is responsible for reading configuration files, creating required directories, \n",
    "    and providing specific configuration objects needed for various components of the project.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Parsed content of the main configuration file.\n",
    "        params (dict): Parsed content of the parameters file.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the ConfigurationManager.\n",
    "\n",
    "        Reads YAML configuration files for main configuration, parameters, and schema. \n",
    "        Also ensures that the root artifacts directory specified in the configuration is created.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): Path to the main configuration YAML file. Default is `CONFIG_FILE_PATH`.\n",
    "            params_filepath (str): Path to the parameters YAML file. Default is `PARAMS_FILE_PATH`.           \n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"\n",
    "        Retrieves the configuration for the data transformation stage.\n",
    "\n",
    "        - Reads the `data_transformation` section of the main configuration file.\n",
    "        - Ensures that the root directory for data transformation outputs exists.\n",
    "        - Creates and returns a `DataTransformationConfig` object with paths for training and testing datasets.\n",
    "\n",
    "        Returns:\n",
    "            DataTransformationConfig: An object containing paths and directories needed for data transformation.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during configuration retrieval or directory creation.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = self.config.data_transformation\n",
    "            create_directories([config.root_dir])\n",
    "            \n",
    "            data_transformation_config = DataTransformationConfig(\n",
    "                root_dir=config.root_dir,\n",
    "                X_train_data_path=config.X_train_data_path,\n",
    "                X_test_data_path=config.X_test_data_path,\n",
    "                y_train_data_path=config.y_train_data_path,\n",
    "                y_test_data_path=config.y_test_data_path\n",
    "            )\n",
    "            return data_transformation_config\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Update the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "from mlproject import logger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoding:\n",
    "    \"\"\"\n",
    "    Applies label encoding to target variables in the training and testing datasets.\n",
    "\n",
    "    This class handles the transformation of categorical target labels into numeric form\n",
    "    using scikit-learn's `LabelEncoder`. The transformed data is saved to the configured \n",
    "    output paths.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for \n",
    "                                           saving the transformed target datasets.\n",
    "\n",
    "    Methods:\n",
    "        apply_label_encoding() -> Tuple[pd.Series, pd.Series]:\n",
    "            Encodes the target labels in the training and testing datasets, saves them, \n",
    "            and returns the encoded series.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the LabelEncoding class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths for \n",
    "                                               saving the transformed target datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def apply_label_encoding(self) -> Tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Applies label encoding to the target variables in the training and testing datasets.\n",
    "\n",
    "        - Reads the target variables from CSV files.\n",
    "        - Encodes the target variables using `LabelEncoder`.\n",
    "        - Saves the transformed training and testing target variables to the configured paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.Series, pd.Series]: Encoded target variables for training and testing datasets.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during label encoding or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            y_train = pd.read_csv('artifacts/data_split/y_train.csv').iloc[:, 0]\n",
    "            y_test = pd.read_csv('artifacts/data_split/y_test.csv').iloc[:, 0]\n",
    "            \n",
    "            labelEncoder = LabelEncoder()\n",
    "\n",
    "            y_train = labelEncoder.fit_transform(y_train)\n",
    "            y_test = labelEncoder.transform(y_test)\n",
    "\n",
    "            y_train = pd.Series(y_train, name='Status')\n",
    "            y_test = pd.Series(y_test, name='Status')\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            y_train.to_csv(self.config.y_train_data_path, index=False)\n",
    "            y_test.to_csv(self.config.y_test_data_path, index=False)\n",
    "            \n",
    "            return y_train, y_test\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoding:\n",
    "    \"\"\"\n",
    "    Applies target mean encoding to a specified categorical feature in the dataset.\n",
    "\n",
    "    This class performs target encoding by replacing categories in a column with \n",
    "    the mean value of the target variable (`y_train`) for that category. The encoding \n",
    "    is applied to both the training and testing datasets, and missing categories in the \n",
    "    test set are handled by assigning the overall mean target value.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for \n",
    "                                           saving the transformed feature datasets.\n",
    "\n",
    "    Methods:\n",
    "        apply_target_encoding() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "            Performs target mean encoding on the `Security_Type` column in the \n",
    "            training and testing datasets, saves the results, and returns the \n",
    "            transformed datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the TargetEncoding class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths \n",
    "                                               for saving the transformed datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def apply_target_encoding(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Applies target mean encoding to the `Security_Type` column in the datasets.\n",
    "\n",
    "        Steps:\n",
    "            - Combines the `Security_Type` column from `X_train` with `y_train` to \n",
    "              calculate the mean target value for each category.\n",
    "            - Maps the calculated mean values to the `Security_Type` column in `X_train`.\n",
    "            - Applies the same mapping to `X_test`, assigning the overall mean target \n",
    "              value to missing categories.\n",
    "            - Saves the transformed training and testing datasets to the configured paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: The transformed `X_train` and `X_test` \n",
    "                                              datasets with target encoding applied.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the encoding process or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X_train = pd.read_csv('artifacts/data_split/X_train.csv')\n",
    "            X_test = pd.read_csv('artifacts/data_split/X_test.csv')\n",
    "            y_train = pd.read_csv(self.config.y_train_data_path)\n",
    "            target_column ='Status'\n",
    "            \n",
    "            # Step 1: Combine `X_train` and `y_train` for mean encoding\n",
    "            security_type_df = pd.concat([X_train['Security_Type'], y_train], axis=1)\n",
    "\n",
    "            # Step 2: Calculate mean target for each category in the training set\n",
    "            security_type_mean = security_type_df.groupby('Security_Type')[target_column].mean()\n",
    "\n",
    "            # Step 3: Map mean encoding to the training set\n",
    "            X_train['Security_Type'] = X_train['Security_Type'].map(security_type_mean)\n",
    "\n",
    "            # Step 4: Map mean encoding to the test set\n",
    "            X_test['Security_Type'] = X_test['Security_Type'].map(security_type_mean)\n",
    "\n",
    "            # Step 5: Handle categories in test set that are missing in training\n",
    "            fallback_value = y_train.mean()  # Overall mean target value\n",
    "            X_test['Security_Type'] = X_test['Security_Type'].fillna(fallback_value)\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_train.to_csv(self.config.X_train_data_path, index = False)\n",
    "            X_test.to_csv(self.config.X_test_data_path, index = False)\n",
    "            \n",
    "            return X_train, X_test\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoding:\n",
    "    \"\"\"\n",
    "    Performs one-hot encoding on specified categorical features in the datasets.\n",
    "\n",
    "    This class applies one-hot encoding to selected columns in the training and \n",
    "    testing datasets to convert categorical variables with two or three unique values \n",
    "    into binary indicator variables. The encoded datasets are saved to configured paths.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for \n",
    "                                           saving the transformed datasets.\n",
    "\n",
    "    Methods:\n",
    "        apply_one_hot_encoding() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "            Performs one-hot encoding on specified categorical columns, ensures consistency \n",
    "            of columns between training and testing datasets, saves the results, and \n",
    "            returns the transformed datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the OneHotEncoding class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths \n",
    "                                               for saving the transformed datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "    \n",
    "    def apply_one_hot_encoding(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Applies one-hot encoding to selected categorical columns in the datasets.\n",
    "\n",
    "        Steps:\n",
    "            - Reads the training (`X_train`) and testing (`X_test`) datasets from configured paths.\n",
    "            - Identifies columns to be one-hot encoded based on a predefined list (`nunique_2_to_3`).\n",
    "            - Applies one-hot encoding to the specified columns in both datasets.\n",
    "            - Ensures that the testing dataset (`X_test`) has the same columns as the \n",
    "              training dataset (`X_train`) by reindexing and filling missing columns with zeros.\n",
    "            - Saves the transformed datasets to the configured paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: The transformed `X_train` and `X_test` datasets \n",
    "                                              with one-hot encoding applied.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the encoding process or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X_train = pd.read_csv(self.config.X_train_data_path)\n",
    "            X_test = pd.read_csv(self.config.X_test_data_path)\n",
    "            nunique_2_to_3 = [\n",
    "                'loan_limit',\n",
    "                'approv_in_adv',\n",
    "                'loan_type',\n",
    "                'Credit_Worthiness',\n",
    "                'open_credit',\n",
    "                'business_or_commercial',\n",
    "                'Neg_ammortization',\n",
    "                'interest_only',\n",
    "                'lump_sum_payment',\n",
    "                'construction_type',\n",
    "                'occupancy_type',\n",
    "                'Secured_by',\n",
    "                'co-applicant_credit_type',\n",
    "                'submission_of_application',\n",
    "                'Security_Type'\n",
    "                ]\n",
    "            \n",
    "            # Remove 'Security_Type' as it will not be one-hot encoded\n",
    "            nunique_2_to_3.remove('Security_Type')\n",
    "\n",
    "            # One-hot encode on X_train and assign back to X_train\n",
    "            X_train = pd.get_dummies(X_train, columns=nunique_2_to_3)\n",
    "\n",
    "            # One-hot encode on X_test and ensure consistent columns with X_train\n",
    "            X_test = pd.get_dummies(X_test, columns=nunique_2_to_3)\n",
    "            X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_train.to_csv(self.config.X_train_data_path, index = False)\n",
    "            X_test.to_csv(self.config.X_test_data_path, index = False)\n",
    "            \n",
    "            return X_train, X_test\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoding:\n",
    "    \"\"\"\n",
    "    Performs frequency encoding on specified categorical features in the datasets.\n",
    "\n",
    "    This class applies frequency encoding to categorical columns in the training and \n",
    "    testing datasets by replacing category values with their frequency counts in the \n",
    "    training data. Transformed datasets are saved to the configured paths.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for \n",
    "                                           saving the transformed datasets.\n",
    "\n",
    "    Methods:\n",
    "        apply_frequency_encoding() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "            Applies frequency encoding to specified columns, ensures consistency between \n",
    "            the training and testing datasets, saves the results, and returns the transformed datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the FrequencyEncoding class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths for saving the \n",
    "                                               transformed datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "    \n",
    "    def apply_frequency_encoding(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Applies frequency encoding to selected categorical columns in the datasets.\n",
    "\n",
    "        Steps:\n",
    "            - Reads the training (`X_train`) and testing (`X_test`) datasets from the configured paths.\n",
    "            - Identifies columns to be frequency encoded based on a predefined list (`greater_than_3`).\n",
    "            - Calculates frequency maps from the training dataset and applies them to both \n",
    "              training and testing datasets.\n",
    "            - Handles unseen categories in the testing dataset by replacing them with a default value (0).\n",
    "            - Saves the transformed datasets to the configured paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: The transformed `X_train` and `X_test` datasets \n",
    "                                              with frequency encoding applied.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the encoding process or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X_train = pd.read_csv(self.config.X_train_data_path)\n",
    "            X_test = pd.read_csv(self.config.X_test_data_path)\n",
    "            greater_than_3 = [\n",
    "                'Gender', 'loan_purpose', 'total_units', 'credit_type', 'age', 'Region'\n",
    "                ]\n",
    "            \n",
    "            for col in greater_than_3:\n",
    "                # Step 1: Calculate frequency map from X_train\n",
    "                frequency_map = X_train[col].value_counts().to_dict()\n",
    "                \n",
    "                # Step 2: Apply frequency map to X_train\n",
    "                X_train[col] = X_train[col].map(frequency_map)\n",
    "                \n",
    "                # Step 3: Apply the same frequency map to X_test\n",
    "                X_test[col] = X_test[col].map(frequency_map)\n",
    "                \n",
    "                # Step 4: Handle categories in X_test not seen in X_train\n",
    "                X_test[col] = X_test[col].fillna(0)  # Replace NaN with 0\n",
    "               \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_train.to_csv(self.config.X_train_data_path, index = False)\n",
    "            X_test.to_csv(self.config.X_test_data_path, index = False)\n",
    "            \n",
    "            return X_train, X_test\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataScaling:\n",
    "    \"\"\"\n",
    "    Applies standard scaling (Z-score normalization) to the features in the training and \n",
    "    testing datasets.\n",
    "\n",
    "    This class scales the numerical features of the training and testing datasets by transforming \n",
    "    them to have a mean of 0 and a standard deviation of 1. The transformed datasets are saved to \n",
    "    the configured paths.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for \n",
    "                                           saving the scaled datasets.\n",
    "\n",
    "    Methods:\n",
    "        apply_standard_scaler() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "            Scales the features of the training and testing datasets using StandardScaler, \n",
    "            saves the transformed datasets, and returns them.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the DataScaling class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths for saving the \n",
    "                                               transformed datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "    \n",
    "    def apply_standard_scaler(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Scales the features in the training and testing datasets using StandardScaler.\n",
    "\n",
    "        The method performs the following steps:\n",
    "            - Reads the training (`X_train`) and testing (`X_test`) datasets from the configured paths.\n",
    "            - Applies standard scaling to the features of both datasets, transforming them to have a \n",
    "              mean of 0 and a standard deviation of 1.\n",
    "            - Saves the scaled datasets back to the configured paths.\n",
    "            - Returns the scaled training and testing datasets as DataFrames.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: The transformed `X_train` and `X_test` datasets with \n",
    "                                              standardized features.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the scaling process or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X_train = pd.read_csv(self.config.X_train_data_path).reset_index(drop=True)\n",
    "            X_test = pd.read_csv(self.config.X_test_data_path).reset_index(drop=True)\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "            X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_train_scaled_df.to_csv(self.config.X_train_data_path, index = False)\n",
    "            X_test_scaled_df.to_csv(self.config.X_test_data_path, index = False)\n",
    "            \n",
    "            return X_train_scaled_df, X_test_scaled_df\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandlingImbalanceDataset:\n",
    "    \"\"\"\n",
    "    Handles class imbalance in the training dataset using ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "    This class is designed to balance the classes in the training dataset by generating synthetic samples \n",
    "    using the ADASYN algorithm. The transformed dataset with balanced classes is saved to the specified \n",
    "    paths.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for saving the balanced \n",
    "                                           training dataset.\n",
    "\n",
    "    Methods:\n",
    "        apply_ADASYN(X_train_scaled_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "            Balances the classes in the training dataset using the ADASYN algorithm.\n",
    "            Saves the transformed dataset with balanced classes and returns it.\n",
    "\n",
    "        Args:\n",
    "            X_train_scaled_df (pd.DataFrame): The scaled training dataset.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: The balanced `X_train` DataFrame and the corresponding \n",
    "                                             balanced target `y_train` Series.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the balancing process or file operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the HandlingImbalanceDataset class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object with file paths for saving the \n",
    "                                               balanced training dataset.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "    \n",
    "    def apply_ADASYN(self, X_train_scaled_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"\n",
    "        Balances the classes in the training dataset using ADASYN (Adaptive Synthetic Sampling).\n",
    "\n",
    "        This method performs the following steps:\n",
    "            - Reads the target variable from the 'y_train_data_path'.\n",
    "            - Combines the scaled features and target into one DataFrame for ADASYN to process.\n",
    "            - Uses the ADASYN algorithm to generate synthetic samples for the minority class, balancing \n",
    "              the class distribution.\n",
    "            - Saves the balanced dataset to the configured paths.\n",
    "            - Returns the balanced `X_train` DataFrame and the balanced target `y_train` Series.\n",
    "\n",
    "        Args:\n",
    "            X_train_scaled_df (pd.DataFrame): Scaled training dataset with features.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.Series]: The balanced `X_train` DataFrame and the corresponding \n",
    "                                             balanced target `y_train` Series.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any error occurs during the balancing process or file operations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # X_train_scaled_df = pd.read_csv(self.config.X_train_data_path)\n",
    "            y_train = pd.read_csv(self.config.y_train_data_path).iloc[:, 0]  # Extract target column\n",
    "        \n",
    "            # Ensure index alignment\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "            \n",
    "            # Concatenate scaled features and target\n",
    "            scaled_data_with_target = pd.concat([X_train_scaled_df, y_train], axis=1)\n",
    "\n",
    "            # Correct column names for clarity\n",
    "            scaled_data_with_target.columns = list(X_train_scaled_df.columns) + ['Status']\n",
    "            \n",
    "            # Applying ADASYN to balance the classes\n",
    "            adasyn = ADASYN(sampling_strategy='minority', random_state=42)\n",
    "            X_resampled, y_resampled = adasyn.fit_resample(X_train_scaled_df, y_train)\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_resampled.to_csv(self.config.X_train_data_path, index = False)\n",
    "            y_resampled.to_csv(self.config.y_train_data_path, index = False)\n",
    "            \n",
    "            return X_resampled, y_resampled\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "    \"\"\"\n",
    "    Performs feature selection by dropping irrelevant or less significant columns from the dataset.\n",
    "\n",
    "    The `FeatureSelection` class processes the resampled training data and test data, removing \n",
    "    specific columns that are deemed unnecessary for model training. The updated datasets are \n",
    "    saved to their respective file paths defined in the `DataTransformationConfig`.\n",
    "\n",
    "    Attributes:\n",
    "        config (DataTransformationConfig): Configuration object containing paths for input \n",
    "        and output data files.\n",
    "\n",
    "    Methods:\n",
    "        select_features:\n",
    "            Drops specified columns from the training and test datasets, saves the updated \n",
    "            datasets to the configured file paths, and returns the modified datasets.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Propagates any exceptions that occur during the feature selection process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureSelection class.\n",
    "\n",
    "        Args:\n",
    "            config (DataTransformationConfig): Configuration object containing paths for \n",
    "            training and test datasets.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "    def select_features(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Drops unnecessary columns from the resampled training data and test data.\n",
    "\n",
    "        Reads the training and test datasets, removes a predefined list of columns, and \n",
    "        saves the resulting datasets back to their configured file paths.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[pd.DataFrame, pd.DataFrame]: Updated training and test datasets after \n",
    "            feature selection.\n",
    "\n",
    "        Steps:\n",
    "            1. Reads the resampled training and test datasets from their respective paths.\n",
    "            2. Removes columns listed in `columns_to_drop03`.\n",
    "            3. Saves the updated datasets to the configured file paths.\n",
    "            4. Returns the modified datasets.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If any errors occur during the feature selection process, they are \n",
    "            raised and propagated.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X_resampled = pd.read_csv(self.config.X_train_data_path).reset_index(drop=True)\n",
    "            X_test = pd.read_csv(self.config.X_test_data_path).reset_index(drop=True)\n",
    "\n",
    "            columns_to_drop03 = [\n",
    "                'ID', 'year', 'term', 'loan_limit_cf', 'loan_limit_ncf','Interest_rate_spread',\n",
    "                'property_value','submission_of_application_not_inst',\n",
    "                'submission_of_application_to_inst', 'construction_type_mh',\n",
    "                'construction_type_sb', 'open_credit_nopc', 'open_credit_opc'\n",
    "                ]\n",
    "\n",
    "            X_resampled.drop(columns_to_drop03, axis=1, inplace=True)\n",
    "            X_test.drop(columns_to_drop03, axis=1, inplace=True)\n",
    "            \n",
    "            # Save transformed data to 'data_transformation' dir\n",
    "            X_resampled.to_csv(self.config.X_train_data_path, index = False)\n",
    "            X_test.to_csv(self.config.X_test_data_path, index = False)\n",
    "            \n",
    "            return X_resampled, X_test\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Update pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-22 02:37:18,249: 35 mlprojectLogger: INFO: common: .yaml file: config\\config.yaml loaded successfully.]\n",
      "[2024-11-22 02:37:18,386: 35 mlprojectLogger: INFO: common: .yaml file: params.yaml loaded successfully.]\n",
      "[2024-11-22 02:37:18,438: 54 mlprojectLogger: INFO: common: Created directory at artifacts]\n",
      "[2024-11-22 02:37:18,439: 54 mlprojectLogger: INFO: common: Created directory at artifacts/data_transformation]\n"
     ]
    }
   ],
   "source": [
    "# Run this here first to check if it works wihout error\n",
    "# If it does then convert it to ml pipeline\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    \n",
    "    y_train, y_test = LabelEncoding(data_transformation_config).apply_label_encoding()\n",
    "    X_train, X_test = TargetEncoding(data_transformation_config).apply_target_encoding()\n",
    "    X_train, X_test = OneHotEncoding(data_transformation_config).apply_one_hot_encoding()\n",
    "    X_train, X_test = FrequencyEncoding(data_transformation_config).apply_frequency_encoding()\n",
    "    X_train_scaled_df, X_test_scaled_df = DataScaling(data_transformation_config).apply_standard_scaler()\n",
    "    X_resampled, y_train = HandlingImbalanceDataset(data_transformation_config).apply_ADASYN(X_train_scaled_df)\n",
    "    X_resampled, X_test = FeatureSelection(data_transformation_config).select_features()\n",
    "        \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in X_train: (105088, 36)\n",
      "Rows in X_test: (19383, 36)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows in X_train: {X_resampled.shape}\")\n",
    "print(f\"Rows in X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in x_train: (105088, 36)\n",
      "Rows in x_test: (19383, 36)\n",
      "Rows in y_train: 105088\n",
      "Rows in y_test: 19383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = pd.read_csv('artifacts/data_transformation/x_train.csv')\n",
    "b = pd.read_csv('artifacts/data_transformation/x_test.csv')\n",
    "c = pd.read_csv('artifacts/data_transformation/y_train.csv').iloc[:, 0]\n",
    "d = pd.read_csv('artifacts/data_transformation/y_test.csv').iloc[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Rows in x_train: {a.shape}\")\n",
    "print(f\"Rows in x_test: {b.shape}\")\n",
    "print(f\"Rows in y_train: {c.shape[0]}\")\n",
    "print(f\"Rows in y_test: {d.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (2958156566.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    t.csv')\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "# Data transform\n",
    "# Check row counts\n",
    "\n",
    "# print(f\"Rows in y_train: {y_train.shape[0]}\")\n",
    "# print(f\"Rows in y_test: {y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in X_train: (105088, 49)\n",
      "Rows in X_test: (19383, 49)\n",
      "Rows in y_train: 105088\n",
      "Rows in y_test: 19383\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "a = pd.read_csv('artifacts/data_transformation/X_train.csv')\n",
    "b = pd.read_csv('artifacts/data_transformation/X_test.csv')\n",
    "c = pd.read_csv('artifacts/data_transformation/y_train.csv').iloc[:, 0]\n",
    "d = pd.read_csv('artifacts/data_transformation/y_test.csv').iloc[:, 0]\n",
    "\n",
    "# SPlit dir\n",
    "print(f\"Rows in X_train: {a.shape}\")\n",
    "print(f\"Rows in X_test: {b.shape}\")\n",
    "\n",
    "print(f\"Rows in y_train: {c.shape[0]}\")\n",
    "print(f\"Rows in y_test: {d.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-22 01:39:40,265: 223 matplotlib.category: INFO: category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.]\n",
      "[2024-11-22 01:39:40,375: 223 matplotlib.category: INFO: category: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_10940\\4110310508.py:4: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=y_train, palette='Set2')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Status', ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# resampled ytrain\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x=y_train, palette='Set2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
